{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa1fce1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242e2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cleanlab\n",
    "from cleanlab.rank import get_label_quality_scores, get_label_quality_ensemble_scores\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, precision_recall_curve, accuracy_score, log_loss, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from eval_metrics import lift_at_k\n",
    "from active_learning_scores import least_confidence\n",
    "from experimental_scores import probability_mass_above_given_label_score\n",
    "\n",
    "# experimental version of label quality ensemble scores with additional weighting schemes\n",
    "from label_quality_ensemble_scores_experimental import get_label_quality_ensemble_scores_experimental\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6dbc04",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc8c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"resnet18\",\n",
    "    \"resnet50d\",\n",
    "    \"efficientnet_b1\",\n",
    "    \"twins_pcpvt_base\",\n",
    "    \"swin_base_patch4_window7_224\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43d0e7",
   "metadata": {},
   "source": [
    "### Dictionaries to map to display names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737ba424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to map to display name\n",
    "\n",
    "method_adjust_pred_probs_display_dict = {\n",
    "    \"self_confidence-False\": \"Self Confidence\",\n",
    "    \"self_confidence-True\": \"Adjusted Self Confidence\",\n",
    "    \"normalized_margin-False\": \"Normalized Margin\",\n",
    "    \"normalized_margin-True\": \"Adjusted Normalized Margin\",\n",
    "    \"confidence_weighted_entropy-False\": \"Confidence Weighted Entropy\",\n",
    "    \"entropy-False\": \"Entropy\",\n",
    "    \"least_confidence-False\": \"Least Confidence\",\n",
    "}\n",
    "\n",
    "model_display_name_dict = {\n",
    "    \"swin_base_patch4_window7_224\": \"Swin Transformer\",\n",
    "    \"twins_pcpvt_base\": \"Twins PCPVT\",\n",
    "    \"efficientnet_b1\": \"EfficientNet-B1\",\n",
    "    \"resnet50d\": \"ResNet-50d\",\n",
    "    \"resnet18\": \"ResNet-18\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c219d08",
   "metadata": {},
   "source": [
    "## Load files from experiments\n",
    "**Note:** we can refactor the code later to make it more concise but for now it reads the .npy files for each dataset within the for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b6b1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 {False, True} <class 'numpy.ndarray'> <class 'numpy.bool_'>\n",
      "resnet50d {False, True} <class 'numpy.ndarray'> <class 'numpy.bool_'>\n",
      "efficientnet_b1 {False, True} <class 'numpy.ndarray'> <class 'numpy.bool_'>\n",
      "twins_pcpvt_base {False, True} <class 'numpy.ndarray'> <class 'numpy.bool_'>\n",
      "swin_base_patch4_window7_224 {False, True} <class 'numpy.ndarray'> <class 'numpy.bool_'>\n",
      "CPU times: user 143 ms, sys: 116 ms, total: 259 ms\n",
      "Wall time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiments = []\n",
    "noxval = \"noxval_\" # change this to \"noxval_\" when trying to benchmark no cross validation\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    #### Andrew Ng DCAI Roman Numerals ####\n",
    "\n",
    "    # read numpy files\n",
    "    numpy_out_folder = f\"./roman-numeral/{noxval}roman-numeral_train_val_dataset_cv_{model}/\"\n",
    "    pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "    labels = np.load(numpy_out_folder + \"noisy_labels.npy\")\n",
    "    images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "    label_errors_mask = np.load(numpy_out_folder + \"label_errors_mask.npy\")\n",
    "    \n",
    "    print(model, set(label_errors_mask), type(label_errors_mask), type(label_errors_mask[0]))\n",
    "    \n",
    "    # store results of experiment\n",
    "    experiment_results = {\n",
    "        \"dataset\": \"roman-numeral\",\n",
    "        \"model\": model,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images,\n",
    "        \"label_errors_mask\": label_errors_mask\n",
    "    }\n",
    "    experiments.append(experiment_results)\n",
    "\n",
    "\n",
    "    #### Cifar-10n-worst\n",
    "\n",
    "    # read numpy files\n",
    "    numpy_out_folder = f\"./cifar-10n-worst/{noxval}cifar-10n-png_noise_type_worst_cv_{model}/\"\n",
    "    pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "    labels = np.load(numpy_out_folder + \"noisy_labels.npy\")\n",
    "    true_labels = np.load(numpy_out_folder + \"true_labels.npy\")\n",
    "    images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "\n",
    "    label_errors_mask = (true_labels != labels) # boolean mask of label errors\n",
    "\n",
    "    # store results of experiment\n",
    "    experiment_results = {\n",
    "        \"dataset\": \"cifar-10n-worst\",\n",
    "        \"model\": model,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images,\n",
    "        \"label_errors_mask\": label_errors_mask\n",
    "    }\n",
    "    experiments.append(experiment_results)\n",
    "\n",
    "\n",
    "    #### Cifar-10n-aggregate\n",
    "\n",
    "    # read numpy files\n",
    "    numpy_out_folder = f\"./cifar-10n-aggregate/{noxval}cifar-10n-png_noise_type_aggre_cv_{model}/\"\n",
    "    pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "    labels = np.load(numpy_out_folder + \"noisy_labels.npy\")\n",
    "    true_labels = np.load(numpy_out_folder + \"true_labels.npy\")\n",
    "    images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "\n",
    "    label_errors_mask = (true_labels != labels) # boolean mask of label errors\n",
    "\n",
    "    # store results of experiment\n",
    "    experiment_results = {\n",
    "        \"dataset\": \"cifar-10n-aggregate\",\n",
    "        \"model\": model,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images,\n",
    "        \"label_errors_mask\": label_errors_mask    \n",
    "    }\n",
    "    experiments.append(experiment_results)\n",
    "\n",
    "\n",
    "    #### Cifar-10\n",
    "\n",
    "    # synthetic noise amount 20% and sparsity 40% (as defined in confident learning paper)\n",
    "\n",
    "    # read numpy files\n",
    "    numpy_out_folder = f\"./cifar-10/{noxval}cifar10_train_dataset_noise_amount_0.2_sparsity_0.4_cv_{model}/\"\n",
    "    pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "    labels = np.load(numpy_out_folder + \"noisy_labels.npy\")\n",
    "    true_labels = np.load(numpy_out_folder + \"true_labels.npy\")\n",
    "    images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "\n",
    "    label_errors_mask = (true_labels != labels) # boolean mask of label errors\n",
    "\n",
    "    # store results of experiment\n",
    "    experiment_results = {\n",
    "        \"dataset\": \"cifar-10\",\n",
    "        \"model\": model,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images,\n",
    "        \"label_errors_mask\": label_errors_mask    \n",
    "    }\n",
    "    experiments.append(experiment_results)\n",
    "\n",
    "\n",
    "    #### Food-101n\n",
    "\n",
    "    # we only have verified labels for ~50K images so we have to evaluate within this subset\n",
    "\n",
    "    # read numpy files\n",
    "    numpy_out_folder = f\"./food-101n/food-101n_cross_val/food-101n_cv_{model}/\"\n",
    "    pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "    labels = np.load(numpy_out_folder + \"noisy_labels.npy\")\n",
    "    images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "\n",
    "    # read verified training labels\n",
    "    path_verified_train = \"./food-101n/verified_train.tsv\"\n",
    "    df_verified_train = pd.read_csv(path_verified_train, sep='\\t')\n",
    "\n",
    "    # instantiate DataFrame with all training data\n",
    "    df_image_paths = pd.DataFrame({\n",
    "        \"class_name/key\": pd.Series(images).map(lambda f: \"/\".join(Path(f).parts[-2:]))\n",
    "    })\n",
    "\n",
    "    # join to append verification_label column\n",
    "    df_image_paths_w_verified = df_image_paths.merge(df_verified_train, on=\"class_name/key\", how=\"left\")\n",
    "\n",
    "    # subset of data with verified labels\n",
    "    verified_subset_mask = ~df_image_paths_w_verified.verification_label.isnull().values\n",
    "\n",
    "    # filter on verified subset\n",
    "    pred_probs = pred_probs[verified_subset_mask]\n",
    "    labels = labels[verified_subset_mask]\n",
    "    images = images[verified_subset_mask]\n",
    "\n",
    "    # boolean mask of label errors\n",
    "    label_errors_mask = df_image_paths_w_verified[\"verification_label\"].values[verified_subset_mask] == 0\n",
    "\n",
    "    # store results of experiment\n",
    "    experiment_results = {\n",
    "        \"dataset\": \"food-101n\",\n",
    "        \"model\": model,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images,\n",
    "        \"label_errors_mask\": label_errors_mask  \n",
    "    }\n",
    "    experiments.append(experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2af7c",
   "metadata": {},
   "source": [
    "## Evaluate all results from individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07d8d3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2880, 10) \n",
      " (2880,) (2880,) (2880,)\n",
      "{False, True} roman-numeral\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-worst\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-aggregate\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2880, 10) \n",
      " (2880,) (2880,) (2880,)\n",
      "{False, True} roman-numeral\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-worst\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-aggregate\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2880, 10) \n",
      " (2880,) (2880,) (2880,)\n",
      "{False, True} roman-numeral\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-worst\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-aggregate\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2880, 10) \n",
      " (2880,) (2880,) (2880,)\n",
      "{False, True} roman-numeral\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-worst\n",
      "<class 'str'> <class 'str'> <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 10) \n",
      " (50000,) (50000,) (50000,)\n",
      "{False, True} cifar-10n-aggregate\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:60\u001b[0m\n",
      "File \u001b[0;32m~/label-error-detection-benchmarks/src/experiments/eval_metrics.py:8\u001b[0m, in \u001b[0;36mlift_at_k\u001b[0;34m(y_true, y_score, k)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Compute Lift at K evaluation metric\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# sort scores\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m sort_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# compute lift for the top k values\u001b[39;00m\n\u001b[1;32m     11\u001b[0m lift_at_k \u001b[38;5;241m=\u001b[39m y_true[sort_indices][\u001b[38;5;241m-\u001b[39mk:]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m/\u001b[39m y_true\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1114\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# args to pass to get_label_quality_scores()\n",
    "score_params = \\\n",
    "[\n",
    "    (\"self_confidence\", False),\n",
    "    (\"self_confidence\", True),\n",
    "    (\"normalized_margin\", False),\n",
    "    (\"normalized_margin\", True),\n",
    "    (\"confidence_weighted_entropy\", False),\n",
    "    (\"magl\", False),\n",
    "]\n",
    "\n",
    "evaluations = []\n",
    "precision_recall_curves = [] # store this separately\n",
    "accuracy_list = []\n",
    "\n",
    "for experiment in experiments:\n",
    "    \n",
    "    # experiment results\n",
    "    dataset = experiment[\"dataset\"]\n",
    "    model = experiment[\"model\"]\n",
    "    pred_probs = experiment[\"pred_probs\"]\n",
    "    labels = experiment[\"labels\"]\n",
    "    images = experiment[\"images\"]\n",
    "    label_errors_target = experiment[\"label_errors_mask\"]\n",
    "    \n",
    "    accuracy = {\n",
    "        \"dataset\": dataset,\n",
    "        \"model\": model,\n",
    "        \"cv_accuracy\": (pred_probs.argmax(axis=1) == labels).mean()\n",
    "    }\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    for score_param in score_params:\n",
    "        \n",
    "        # compute scoring method\n",
    "        method, adjust_pred_probs = score_param    \n",
    "    \n",
    "        if \"magl\" in method:\n",
    "            label_quality_scores = probability_mass_above_given_label_score(labels, pred_probs, adjust_pred_probs=adjust_pred_probs, alpha=0.99)\n",
    "        else:\n",
    "            label_quality_scores = get_label_quality_scores(labels=labels, pred_probs=pred_probs, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "            \n",
    "        # compute accuracy of detecting label errors\n",
    "        auroc = roc_auc_score(label_errors_target, 1 - label_quality_scores)\n",
    "\n",
    "        # compute precision-recall curve using label quality scores\n",
    "        precision, recall, thresholds = precision_recall_curve(label_errors_target, 1 - label_quality_scores)\n",
    "\n",
    "        # compute prc auc scores\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        # lift at K where K = number of label errors\n",
    "        lift_at_num_label_errors = lift_at_k(label_errors_target, 1 - label_quality_scores, k=label_errors_target.sum())\n",
    "        \n",
    "        # lift at k=100\n",
    "        lift_at_100 = lift_at_k(label_errors_target, 1 - label_quality_scores, k=100)\n",
    "\n",
    "        evaluation_results = {\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model,\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc,\n",
    "            \"auprc\": auprc,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors,\n",
    "            \"lift_at_100\": lift_at_100,\n",
    "        }\n",
    "\n",
    "        # store evaluation results\n",
    "        evaluations.append(evaluation_results)\n",
    "        \n",
    "        precision_recall_curve_results = {\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model,\n",
    "            \"label_quality_scores\": label_quality_scores,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"thresholds\": thresholds\n",
    "        }\n",
    "        \n",
    "        # store precision-recall curve results\n",
    "        precision_recall_curves.append(precision_recall_curve_results)\n",
    "        \n",
    "\n",
    "    #### active learning scores to use as comparison\n",
    "    \n",
    "    al_scoring_funcs = {\n",
    "        \"entropy\": get_normalized_entropy,\n",
    "        \"least_confidence\": least_confidence\n",
    "    }\n",
    "\n",
    "    for al_method in al_scoring_funcs.keys():\n",
    "        \n",
    "        # active learning scoring function\n",
    "        scoring_func = al_scoring_funcs[al_method]\n",
    "    \n",
    "        # score\n",
    "        al_scores = scoring_func(pred_probs)\n",
    "\n",
    "        # compute accuracy of detecting label errors\n",
    "        auroc = roc_auc_score(label_errors_target, al_scores)\n",
    "\n",
    "        # lift at K where K = number of label errors\n",
    "        lift_at_num_label_errors = lift_at_k(label_errors_target, al_scores, k=label_errors_target.sum())\n",
    "        \n",
    "        # lift at k=100\n",
    "        lift_at_100 = lift_at_k(label_errors_target, al_scores, k=100)\n",
    "        \n",
    "\n",
    "        evaluation_results = {\n",
    "            \"method\": al_method,\n",
    "            \"adjust_pred_probs\": False,\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model,\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors,\n",
    "            \"lift_at_100\": lift_at_100,\n",
    "        }\n",
    "\n",
    "        # store evaluation results\n",
    "        evaluations.append(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation accuracy\n",
    "df_cv_accuracy = pd.DataFrame(accuracy_list)\n",
    "\n",
    "df_cv_accuracy_pivot = (\n",
    "    pd.pivot_table(\n",
    "        df_cv_accuracy, values=\"cv_accuracy\", index=[\"model\"], columns=[\"dataset\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"roman-numeral\", ascending=False)\n",
    ")\n",
    "\n",
    "df_cv_accuracy_pivot[\"model\"] = df_cv_accuracy_pivot.model.map(\n",
    "    lambda x: model_display_name_dict[x]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3370edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_accuracy_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9828ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master table with AUROC and Lift at K evaluation metrics for all methods, datasets, and models\n",
    "df_evaluations = pd.DataFrame(evaluations)\n",
    "\n",
    "# append cv accuracy\n",
    "df_evaluations = df_evaluations.merge(df_cv_accuracy, how=\"left\", on=[\"dataset\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb237b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations[\"method_adjust_pred_probs\"] = (\n",
    "    df_evaluations.method + \"-\" + df_evaluations.adjust_pred_probs.astype(str)\n",
    ")\n",
    "df_evaluations[\"dataset_model\"] = df_evaluations.dataset + \" | \" + df_evaluations.model\n",
    "\n",
    "df_evaluations[\"scoring_method\"] = df_evaluations.method_adjust_pred_probs.map(\n",
    "    lambda x: method_adjust_pred_probs_display_dict[x]\n",
    ")\n",
    "df_evaluations[\"model_name\"] = df_evaluations.model.map(\n",
    "    lambda x: model_display_name_dict[x]\n",
    ")\n",
    "\n",
    "\n",
    "df_evaluations[\"model_name_w_acc\"] = df_evaluations.model_name + \" (\" + df_evaluations.cv_accuracy.round(4).astype(str) + \") \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_evaluations.to_csv(\"evaluation_all_experiments.csv\")\n",
    "df_evaluations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e53b9",
   "metadata": {},
   "source": [
    "### Just auroc values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f62f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_auroc = (\n",
    "    pd.pivot_table(\n",
    "        df_evaluations,\n",
    "        values=\"auroc\",\n",
    "        index=[\"dataset\", \"model_name\", \"model_name_w_acc\"],\n",
    "        columns=[\"scoring_method\"],\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"dataset\", \"model_name\"])\n",
    ")\n",
    "\n",
    "df_evaluations_lift_at_num_errors = (\n",
    "    pd.pivot_table(\n",
    "        df_evaluations,\n",
    "        values=\"lift_at_num_label_errors\",\n",
    "        index=[\"dataset\", \"model_name\", \"model_name_w_acc\"],\n",
    "        columns=[\"scoring_method\"],\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"dataset\", \"model_name\"])\n",
    ")\n",
    "\n",
    "df_evaluations_lift_at_100 = (\n",
    "    pd.pivot_table(\n",
    "        df_evaluations,\n",
    "        values=\"lift_at_100\",\n",
    "        index=[\"dataset\", \"model_name\", \"model_name_w_acc\"],\n",
    "        columns=[\"scoring_method\"],\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"dataset\", \"model_name\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_auroc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54c770",
   "metadata": {},
   "source": [
    "## AUROC Plots\n",
    "AUROC for LED achieved by label quality scores for each dataset and model. Models are ordered by accuracy on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointer\n",
    "df = df_evaluations_auroc\n",
    "df['dataset_model'] = df.apply(lambda x: x['dataset'] + ' | ' + x['model_name_w_acc'],axis=1)\n",
    "\n",
    "# Draw plot\n",
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "\n",
    "s = 60\n",
    "alpha = 0.9\n",
    "marker = \"o\"\n",
    "\n",
    "s0 = plt.scatter(\n",
    "    df[\"Confidence Weighted Entropy\"], df.index, s=s, alpha=alpha, marker=marker\n",
    ")\n",
    "s1 = plt.scatter(df[\"Self Confidence\"], df.index, s=s, alpha=alpha, marker=marker)\n",
    "s2 = plt.scatter(df[\"Adjusted Self Confidence\"], df.index, s=s, alpha=alpha, marker=marker)\n",
    "s3 = plt.scatter(\n",
    "    df[\"Normalized Margin\"], df.index, s=s, alpha=alpha, marker=marker\n",
    ")\n",
    "s4 = plt.scatter(\n",
    "    df[\"Adjusted Normalized Margin\"], df.index, s=s, alpha=alpha, marker=marker\n",
    ")\n",
    "# s5 = plt.scatter(df[\"entropy-False\"], df.index, s=s, alpha=alpha, marker=marker)\n",
    "# s6 = plt.scatter(df[\"least_confidence-False\"], df.index, s=s, alpha=alpha, marker=marker)\n",
    "\n",
    "# for x, y, tex in zip(df[\"confidence_weighted_entropy-False\"], df.index, df[\"confidence_weighted_entropy-False\"]):\n",
    "#     t = plt.text(x, y, round(tex, 1), horizontalalignment='center',\n",
    "#                  verticalalignment='center', fontdict={'color':'white'})\n",
    "\n",
    "plt.title(\"AUROC\", fontsize=18)\n",
    "plt.yticks(df.index, df.dataset_model)\n",
    "plt.legend(\n",
    "    (s0, s1, s2, s3, s4),\n",
    "    (\n",
    "        \"Confidence Weighted Entropy (False)\",\n",
    "        \"Self Confidence (False)\",\n",
    "        \"Self Confidence (True)\",\n",
    "        \"Normalized Margin (False)\",\n",
    "        \"Normalized Margin (True)\",\n",
    "    ),\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 1.2),\n",
    "    ncol=3,\n",
    "    fancybox=True,\n",
    "    shadow=True,\n",
    "    fontsize=12,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9210254",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Lift at # Errors\"\n",
    "\n",
    "# pointer\n",
    "df = df_evaluations_lift_at_num_errors.sort_values(by=[\"dataset\", \"Self Confidence\"])\n",
    "df[\"dataset_model\"] = df.dataset + \" | \" + df.model_name_w_acc\n",
    "\n",
    "labels = df[\"dataset_model\"].tolist()\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "\n",
    "s0 = df[\"Confidence Weighted Entropy\"].tolist()\n",
    "s1 = df[\"Self Confidence\"].tolist()\n",
    "s2 = df[\"Adjusted Self Confidence\"].tolist()\n",
    "s3 = df[\"Normalized Margin\"].tolist()\n",
    "s4 = df[\"Adjusted Normalized Margin\"].tolist()\n",
    "\n",
    "\n",
    "jf = 0.15 # jitter factor\n",
    "markersize = 8\n",
    "alpha = 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.plot(s0, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Confidence Weighted Entropy\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s1, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Self Confidence\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s2, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Adjusted Self Confidence\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s3, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Normalized Margin\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s4, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Adjusted Normalized Margin\", markersize=markersize, alpha=alpha)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_xlabel(\"\")\n",
    "ax.set_title(title, fontsize=24, fontweight=\"bold\")\n",
    "ax.set_yticks(x, labels)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Lift at 100\"\n",
    "\n",
    "# pointer\n",
    "df = df_evaluations_lift_at_100.sort_values(by=[\"dataset\", \"Confidence Weighted Entropy\"])\n",
    "df[\"dataset_model\"] = df.dataset + \" | \" + df.model_name_w_acc\n",
    "\n",
    "labels = df[\"dataset_model\"].tolist()\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "\n",
    "s0 = df[\"Confidence Weighted Entropy\"].tolist()\n",
    "s1 = df[\"Self Confidence\"].tolist()\n",
    "s2 = df[\"Adjusted Self Confidence\"].tolist()\n",
    "s3 = df[\"Normalized Margin\"].tolist()\n",
    "s4 = df[\"Adjusted Normalized Margin\"].tolist()\n",
    "\n",
    "\n",
    "jf = 0.15 # jitter factor\n",
    "markersize = 8\n",
    "alpha = 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.plot(s0, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Confidence Weighted Entropy\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s1, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Self Confidence\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s2, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Adjusted Self Confidence\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s3, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Normalized Margin\", markersize=markersize, alpha=alpha)\n",
    "_ = ax.plot(s4, x + np.random.uniform(-jf, jf), marker=\"o\", linestyle=\"None\", label=\"Adjusted Normalized Margin\", markersize=markersize, alpha=alpha)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_xlabel(\"\")\n",
    "ax.set_title(title, fontsize=24, fontweight=\"bold\")\n",
    "ax.set_yticks(x, labels)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634ad5e",
   "metadata": {},
   "source": [
    "### Precision/recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for data in precision_recall_curves:\n",
    "\n",
    "    # get data needed to plot precision-recall curve\n",
    "    method = data[\"method\"]\n",
    "    adjust_pred_probs = data[\"adjust_pred_probs\"]\n",
    "    dataset = data[\"dataset\"]\n",
    "    model = data[\"model\"]\n",
    "    label_quality_scores = data[\"label_quality_scores\"]\n",
    "    precision = data[\"precision\"]\n",
    "    recall = data[\"recall\"]\n",
    "    thresholds = data[\"thresholds\"]\n",
    "\n",
    "    # save to DataFrame\n",
    "    # ignore last precision, recall value because it's always 1, 0 respectively with no corresponding threshold\n",
    "    # https://stackoverflow.com/questions/31639016/in-scikits-precision-recall-curve-why-does-thresholds-have-a-different-dimensi\n",
    "    df_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"precision\": precision[:-1],\n",
    "            \"recall\": recall[:-1],\n",
    "            \"thresholds\": thresholds,\n",
    "            \"model\": model,\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_list.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbad82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in df_list:\n",
    "    precision = data[\"precision\"]\n",
    "    recall = data[\"recall\"]\n",
    "    method = data[\"method\"]\n",
    "    adjust_pred_probs = data[\"adjust_pred_probs\"]\n",
    "    plt.plot(recall, precision, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "\n",
    "# TODO: Figure this out\n",
    "\n",
    "# # plot single dot (precision, recall) for each filter_by option\n",
    "# for index, row in df_filter_by.iterrows():\n",
    "#     filter_by = row[\"filter_by\"]\n",
    "#     precision = row[\"precision\"]\n",
    "#     recall = row[\"recall\"]\n",
    "#     plt.plot(recall, precision, marker=\"o\", markersize=10, label=filter_by)\n",
    "\n",
    "plt.xlabel(\"Recall\", fontsize=14)\n",
    "plt.ylabel(\"Precision\", fontsize=14)\n",
    "plt.title(\"Precision-Recall Curve: Label Error Detection on CIFAR-10N-Worst \\n Model: swin_base_patch4_window7_224\", fontsize=20, fontweight=\"bold\")\n",
    "# plt.suptitle(\"\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480f347",
   "metadata": {},
   "source": [
    "## Evaluate results from ensemble models\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_model_output = {}\n",
    "\n",
    "for i, experiment in enumerate(experiments):\n",
    "\n",
    "    # experiment results\n",
    "    dataset = experiment[\"dataset\"]\n",
    "    model = experiment[\"model\"]\n",
    "    pred_probs = experiment[\"pred_probs\"]\n",
    "    labels = experiment[\"labels\"]\n",
    "    images = experiment[\"images\"]\n",
    "    label_errors_target = experiment[\"label_errors_mask\"]\n",
    "\n",
    "    # check\n",
    "    if dataset not in dataset_model_output.keys():\n",
    "\n",
    "        # init list of pred_probs and labels\n",
    "        dataset_model_output[dataset] = {}\n",
    "        dataset_model_output[dataset][\"pred_probs_list\"] = []\n",
    "        dataset_model_output[dataset][\"labels_list\"] = []\n",
    "        dataset_model_output[dataset][\"images_list\"] = []\n",
    "        dataset_model_output[dataset][\"label_errors_target_list\"] = []\n",
    "\n",
    "    # store model output on dataset as key\n",
    "    dataset_model_output[dataset][\"pred_probs_list\"].append(pred_probs)\n",
    "    dataset_model_output[dataset][\"labels_list\"].append(labels)\n",
    "    dataset_model_output[dataset][\"images_list\"].append(images)\n",
    "    dataset_model_output[dataset][\"label_errors_target_list\"].append(\n",
    "        label_errors_target\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6d24d",
   "metadata": {},
   "source": [
    "### Evaluate ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# args to pass to get_label_quality_scores()\n",
    "score_params = \\\n",
    "[\n",
    "    (\"self_confidence\", False),\n",
    "    (\"self_confidence\", True),\n",
    "    (\"normalized_margin\", False),\n",
    "    (\"normalized_margin\", True),\n",
    "    (\"confidence_weighted_entropy\", False)\n",
    "]\n",
    "\n",
    "\n",
    "ensemble_evaluations = []\n",
    "\n",
    "dataset_best_weights = []\n",
    "\n",
    "for dataset_key in dataset_model_output.keys():\n",
    "\n",
    "    # get list of pred_probs, labels for dataset\n",
    "    pred_probs_list = dataset_model_output[dataset_key][\"pred_probs_list\"]\n",
    "    labels_list = dataset_model_output[dataset_key][\"labels_list\"]\n",
    "    images_list = dataset_model_output[dataset_key][\"images_list\"]\n",
    "    label_errors_target_list = dataset_model_output[dataset_key][\"label_errors_target_list\"]\n",
    "    \n",
    "    # use for sanity check (noisy labels and images from each model should be the same because they were generated from the same cross-val procedure\n",
    "    for i, (labels_temp, images_temp) in enumerate(zip(labels_list, images_list)):\n",
    "\n",
    "        if i == 0:\n",
    "            labels_temp_previous = copy.deepcopy(labels_temp)\n",
    "            images_temp_previous = copy.deepcopy(images_temp)       \n",
    "        else:\n",
    "            assert (labels_temp_previous == labels_temp).all()\n",
    "            assert (images_temp_previous == images_temp).all()    \n",
    "    \n",
    "    # take the first (the others are the same)\n",
    "    labels = labels_list[0]\n",
    "    label_errors_target = label_errors_target_list[0]\n",
    "    \n",
    "    # compute accuracy\n",
    "    accuracy_list = []\n",
    "    for pred_probs in pred_probs_list:\n",
    "        \n",
    "        # accuracy of single model\n",
    "        accuracy = (pred_probs.argmax(axis=1) == labels).mean()\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "    # accuracy weights\n",
    "    acc_weights = np.array(accuracy_list) / sum(accuracy_list)    \n",
    "    \n",
    "    # average predictions\n",
    "    pred_probs_avg = sum(pred_probs_list) / len(pred_probs_list)\n",
    "    \n",
    "    #### can refactor below to a function that accepts weights and pred_probs_list\n",
    "    \n",
    "    # accuracy-weighted predictions\n",
    "    pred_probs_avg_acc_weighted = sum([acc_weights[i] * p for i, p in enumerate(pred_probs_list)])\n",
    "    \n",
    "\n",
    "    #### find best t in T for exp-log-loss weighting\n",
    "    T = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 2e2]\n",
    "\n",
    "    pred_probs_avg_log_loss_weighted = None\n",
    "    inv_log_loss_weights = None\n",
    "    best_eval_log_loss = float(\"inf\")\n",
    "    best_t = None\n",
    "\n",
    "    for t in T:\n",
    "\n",
    "        log_loss_list = []\n",
    "\n",
    "        # pred_probs for each model\n",
    "        for pred_probs in pred_probs_list:\n",
    "            log_loss_ = np.exp(t * (-log_loss(labels, pred_probs)))\n",
    "            log_loss_list.append(log_loss_)\n",
    "\n",
    "        # weights using log loss\n",
    "        inv_log_loss_weights_temp = np.array(log_loss_list) / sum(log_loss_list)\n",
    "\n",
    "        # weighted average\n",
    "        pred_probs_avg_log_loss_weighted_temp = sum([inv_log_loss_weights_temp[i] * p for i, p in enumerate(pred_probs_list)])\n",
    "\n",
    "        # evaluate log_loss with this weighted average\n",
    "        eval_log_loss = log_loss(labels, pred_probs_avg_log_loss_weighted_temp)\n",
    "\n",
    "        # check if this is the best eval_log_loss so far\n",
    "        if best_eval_log_loss > eval_log_loss:\n",
    "            best_eval_log_loss = eval_log_loss\n",
    "            best_t = t\n",
    "            pred_probs_avg_log_loss_weighted = pred_probs_avg_log_loss_weighted_temp.copy()\n",
    "            inv_log_loss_weights = inv_log_loss_weights_temp.copy()    \n",
    "\n",
    "    df_temp = pd.DataFrame({\n",
    "        \"dataset\": dataset_key,\n",
    "        \"models\": models,\n",
    "        \"best_t\": best_t,\n",
    "        \"best_eval_log_loss\": best_eval_log_loss,\n",
    "        \"inv_log_loss_weights\": inv_log_loss_weights,\n",
    "    })\n",
    "    \n",
    "    # save the weights for analysis later\n",
    "    dataset_best_weights.append(df_temp)\n",
    "            \n",
    "    print()\n",
    "    print(dataset_key)\n",
    "    print(best_eval_log_loss)\n",
    "    print(inv_log_loss_weights)\n",
    "    print(pred_probs_avg_log_loss_weighted)\n",
    "    \n",
    "    \n",
    "    #### label quality scoring\n",
    "    \n",
    "    for score_param in score_params:\n",
    "        \n",
    "        # label quality scoring method\n",
    "        method, adjust_pred_probs = score_param\n",
    "    \n",
    "        # compute scores\n",
    "        \n",
    "        # use average pred_probs\n",
    "        label_quality_scores_avg = get_label_quality_scores(labels=labels, pred_probs=pred_probs_avg, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "        \n",
    "        # use average pred_probs weighted by accuracy\n",
    "        label_quality_scores_avg_acc_weighted = get_label_quality_scores(labels=labels, pred_probs=pred_probs_avg_acc_weighted, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "        \n",
    "        # use average pred_probs weighted by log loss\n",
    "        label_quality_scores_avg_log_loss_weighted = get_label_quality_scores(labels=labels, pred_probs=pred_probs_avg_log_loss_weighted, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "        \n",
    "        # use pred_probs_list (weighted by accuracy)\n",
    "        label_quality_scores_agg_acc = get_label_quality_ensemble_scores_experimental(\n",
    "            labels=labels, \n",
    "            pred_probs_list=pred_probs_list, \n",
    "            method=method, \n",
    "            adjust_pred_probs=adjust_pred_probs, \n",
    "            verbose=0,\n",
    "            weight_ensemble_members_by=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        # use pred_probs_list (uniform_weights)\n",
    "        label_quality_scores_agg_uni = get_label_quality_ensemble_scores_experimental(\n",
    "            labels=labels, \n",
    "            pred_probs_list=pred_probs_list, \n",
    "            method=method, \n",
    "            adjust_pred_probs=adjust_pred_probs,\n",
    "            verbose=0,\n",
    "            weight_ensemble_members_by=\"uniform\"\n",
    "        )\n",
    "        \n",
    "        # use pred_probs_list (weight by inverse log loss)\n",
    "        label_quality_scores_agg_log_loss = get_label_quality_ensemble_scores_experimental(\n",
    "            labels=labels, \n",
    "            pred_probs_list=pred_probs_list, \n",
    "            method=method, \n",
    "            adjust_pred_probs=adjust_pred_probs,\n",
    "            verbose=0, \n",
    "            weight_ensemble_members_by=\"custom\",\n",
    "            custom_weights=inv_log_loss_weights # custom weights!\n",
    "        )        \n",
    "        \n",
    "        # compute accuracy of detecting label errors\n",
    "        auroc_avg = roc_auc_score(label_errors_target, 1 - label_quality_scores_avg)\n",
    "        auroc_avg_acc_weighted = roc_auc_score(label_errors_target, 1 - label_quality_scores_avg_acc_weighted)\n",
    "        auroc_avg_log_loss_weighted = roc_auc_score(label_errors_target, 1 - label_quality_scores_avg_log_loss_weighted)        \n",
    "        \n",
    "        auroc_agg_acc = roc_auc_score(label_errors_target, 1 - label_quality_scores_agg_acc)\n",
    "        auroc_agg_uni = roc_auc_score(label_errors_target, 1 - label_quality_scores_agg_uni)\n",
    "        auroc_agg_log_loss = roc_auc_score(label_errors_target, 1 - label_quality_scores_agg_log_loss)        \n",
    "        \n",
    "        # lift at K where K = number of label errors\n",
    "        lift_at_num_label_errors_avg = lift_at_k(label_errors_target, 1 - label_quality_scores_avg, k=label_errors_target.sum())\n",
    "        lift_at_num_label_errors_avg_acc_weighted = lift_at_k(label_errors_target, 1 - label_quality_scores_avg_acc_weighted, k=label_errors_target.sum())\n",
    "        lift_at_num_label_errors_avg_log_loss_weighted = lift_at_k(label_errors_target, 1 - label_quality_scores_avg_log_loss_weighted, k=label_errors_target.sum())\n",
    "        \n",
    "        lift_at_num_label_errors_agg_acc = lift_at_k(label_errors_target, 1 - label_quality_scores_agg_acc, k=label_errors_target.sum())\n",
    "        lift_at_num_label_errors_agg_uni = lift_at_k(label_errors_target, 1 - label_quality_scores_agg_uni, k=label_errors_target.sum())\n",
    "        lift_at_num_label_errors_agg_log_loss = lift_at_k(label_errors_target, 1 - label_quality_scores_agg_log_loss, k=label_errors_target.sum())        \n",
    "        \n",
    "        # lift at k=100\n",
    "        lift_at_100_avg = lift_at_k(label_errors_target, 1 - label_quality_scores_avg, k=100)\n",
    "        lift_at_100_avg_acc_weighted = lift_at_k(label_errors_target, 1 - label_quality_scores_avg_acc_weighted, k=100)\n",
    "        lift_at_100_avg_log_loss_weighted = lift_at_k(label_errors_target, 1 - label_quality_scores_avg_log_loss_weighted, k=100)\n",
    "        \n",
    "        lift_at_100_agg_acc = lift_at_k(label_errors_target, 1 - label_quality_scores_agg_acc, k=100)\n",
    "        lift_at_100_agg_uni = lift_at_k(label_errors_target, 1 - label_quality_scores_agg_uni, k=100)\n",
    "        lift_at_100_agg_log_loss = lift_at_k(label_errors_target, 1 - label_quality_scores_agg_log_loss, k=100)\n",
    "\n",
    "        ensemble_evaluation_results_avg = {\n",
    "            \"ensemble_method\": \"avg_pred_probs\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset_key,\n",
    "            \"model\": \"ensemble\",\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc_avg,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors_avg,\n",
    "            \"lift_at_100\": lift_at_100_avg\n",
    "        }\n",
    "        \n",
    "        ensemble_evaluation_results_avg_acc_weighted = {\n",
    "            \"ensemble_method\": \"avg_pred_probs_weighted_by_accuracy\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset_key,\n",
    "            \"model\": \"ensemble\",\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc_avg_acc_weighted,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors_avg_acc_weighted,\n",
    "            \"lift_at_100\": lift_at_100_avg_acc_weighted\n",
    "        }        \n",
    "        \n",
    "        ensemble_evaluation_results_avg_log_loss_weighted = {\n",
    "            \"ensemble_method\": \"avg_pred_probs_weighted_by_inv_log_loss\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset_key,\n",
    "            \"model\": \"ensemble\",\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc_avg_log_loss_weighted,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors_avg_log_loss_weighted,\n",
    "            \"lift_at_100\": lift_at_100_avg_log_loss_weighted\n",
    "        }                \n",
    "        \n",
    "        ensemble_evaluation_results_agg_acc = {\n",
    "            \"ensemble_method\": \"avg_scores_weighted_by_accuracy\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset_key,\n",
    "            \"model\": \"ensemble\",\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc_agg_acc,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors_agg_acc,\n",
    "            \"lift_at_100\": lift_at_100_agg_acc\n",
    "        }\n",
    "        \n",
    "        ensemble_evaluation_results_agg_uni = {\n",
    "            \"ensemble_method\": \"avg_scores\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset_key,\n",
    "            \"model\": \"ensemble\",\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc_agg_uni,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors_agg_uni,\n",
    "            \"lift_at_100\": lift_at_100_agg_uni\n",
    "        }\n",
    "        \n",
    "        ensemble_evaluation_results_agg_log_loss = {\n",
    "            \"ensemble_method\": \"avg_scores_weighted_by_inv_log_loss\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": dataset_key,\n",
    "            \"model\": \"ensemble\",\n",
    "            \"dataset_num_samples\": labels.shape[0],\n",
    "            \"dataset_num_label_errors\": label_errors_target.sum(),\n",
    "            \"auroc\": auroc_agg_log_loss,\n",
    "            \"lift_at_num_label_errors\": lift_at_num_label_errors_agg_log_loss,\n",
    "            \"lift_at_100\": lift_at_100_agg_log_loss\n",
    "        }\n",
    "        \n",
    "        # store evaluation results\n",
    "        ensemble_evaluations.append(ensemble_evaluation_results_avg)\n",
    "        ensemble_evaluations.append(ensemble_evaluation_results_avg_acc_weighted)\n",
    "        ensemble_evaluations.append(ensemble_evaluation_results_avg_log_loss_weighted)\n",
    "        \n",
    "        ensemble_evaluations.append(ensemble_evaluation_results_agg_acc)\n",
    "        ensemble_evaluations.append(ensemble_evaluation_results_agg_uni)\n",
    "        ensemble_evaluations.append(ensemble_evaluation_results_agg_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_ensemble = pd.DataFrame(ensemble_evaluations)\n",
    "\n",
    "df_evaluations_ensemble[\"method_adjust_pred_probs\"] = (\n",
    "    df_evaluations_ensemble.method\n",
    "    + \"-\"\n",
    "    + df_evaluations_ensemble.adjust_pred_probs.astype(str)\n",
    ")\n",
    "df_evaluations_ensemble[\"dataset_model\"] = (\n",
    "    df_evaluations_ensemble.dataset + \" | \" + df_evaluations_ensemble.model\n",
    ")\n",
    "\n",
    "df_evaluations_ensemble[\n",
    "    \"scoring_method\"\n",
    "] = df_evaluations_ensemble.method_adjust_pred_probs.map(\n",
    "    lambda x: method_adjust_pred_probs_display_dict[x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_ensemble.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09444bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_ensemble.groupby(\"ensemble_method\")[\"ensemble_method\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74754480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_ensemble_pivot = pd.pivot_table(\n",
    "    df_evaluations_ensemble,\n",
    "    values=\"auroc\",\n",
    "    index=[\"scoring_method\", \"ensemble_method\"],\n",
    "    columns=[\"dataset\"],\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab700bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_ensemble_pivot.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68360c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_ensemble_auroc = (\n",
    "    pd.pivot_table(\n",
    "        df_evaluations_ensemble,\n",
    "        values=\"auroc\",\n",
    "        index=[\"dataset\", \"scoring_method\"],\n",
    "        columns=[\"ensemble_method\"],\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"dataset\", \"scoring_method\"])\n",
    ")\n",
    "\n",
    "df_evaluations_ensemble_lift_at_num_errors = (\n",
    "    pd.pivot_table(\n",
    "        df_evaluations_ensemble,\n",
    "        values=\"lift_at_num_label_errors\",\n",
    "        index=[\"dataset\", \"scoring_method\"],\n",
    "        columns=[\"ensemble_method\"],\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"dataset\", \"scoring_method\"])\n",
    ")\n",
    "\n",
    "df_evaluations_ensemble_lift_at_100 = (\n",
    "    pd.pivot_table(\n",
    "        df_evaluations_ensemble,\n",
    "        values=\"lift_at_100\",\n",
    "        index=[\"dataset\", \"scoring_method\"],\n",
    "        columns=[\"ensemble_method\"],\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"dataset\", \"scoring_method\"])\n",
    ")\n",
    "\n",
    "df_evaluations_ensemble_auroc[\"dataset_scoring_method\"] = \\\n",
    "    df_evaluations_ensemble_auroc.dataset + \" | \" + df_evaluations_ensemble_auroc.scoring_method\n",
    "\n",
    "df_evaluations_ensemble_lift_at_num_errors[\"dataset_scoring_method\"] = \\\n",
    "    df_evaluations_ensemble_lift_at_num_errors.dataset + \" | \" + df_evaluations_ensemble_lift_at_num_errors.scoring_method\n",
    "\n",
    "df_evaluations_ensemble_lift_at_100[\"dataset_scoring_method\"] = \\\n",
    "    df_evaluations_ensemble_lift_at_100.dataset + \" | \" + df_evaluations_ensemble_lift_at_100.scoring_method\n",
    "\n",
    "df_evaluations_ensemble_lift_at_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_ensemble_auroc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e466b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
